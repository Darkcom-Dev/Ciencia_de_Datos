{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL OPTIMIZATION\n",
    "In the previous chapter we built our first supervised learning model. We now\n",
    "want to improve its accuracy and reduce the effects of overfitting. A good\n",
    "place to start is modifying the model’s hyperparameters.\n",
    "Without changing any other hyperparameters, let’s first start by modifying\n",
    "max_depth from “30” to “5.” The model now generates the following results:\n",
    "# Results will differ due to the randomized data split\n",
    "Training Set Mean Absolute Error: 129412.51\n",
    "Although the mean absolute error of the training set is higher, this helps\n",
    "reduce the problem of overfitting and should improve the results of the test\n",
    "data. Another step to optimize the model is to add more trees. If we set\n",
    "n_estimators to 250, we see this result:\n",
    "# Results will differ as per the randomized data split\n",
    "Training Set Mean Absolute Error: 118130.46\n",
    "Test Set Mean Absolute Error: 159886.32\n",
    "This second optimization reduces the training set’s absolute error rate by\n",
    "approximately $11,000 and we now have a smaller gap between our training\n",
    "and test results for mean absolute error.\n",
    "Together, these two optimizations underline the importance of maximizing\n",
    "and understanding the impact of individual hyperparameters. If you decide to\n",
    "replicate this supervised machine learning model at home, I recommend that\n",
    "you test modifying each of the hyperparameters individually and analyze\n",
    "their impact on mean absolute error. In addition, you will notice changes in\n",
    "the machine’s processing time based on the hyperparameters selected. For\n",
    "instance, setting max_depth to “5” reduces total processing time compared to\n",
    "when it was set to “30” because the maximum number of branch layers are\n",
    "significantly less. Processing speed and resources will become an important\n",
    "consideration as you move on to working with larger datasets.\n",
    "Another important optimization technique is feature selection. As you willrecall, we removed nine features while scrubbing our dataset. Now might be\n",
    "a good time to reconsider those features and analyze whether they have an\n",
    "effect on the overall accuracy of the model. “SellerG” would be an interesting\n",
    "feature to add to the model because the real estate company selling the\n",
    "property could have some impact on the final selling price.\n",
    "Alternatively, dropping features from the current model may reduce\n",
    "processing time without having a significant effect on accuracy—or may\n",
    "even improve accuracy. To select features effectively, it is best to isolate\n",
    "feature modifications and analyze the results, rather than applying various\n",
    "changes at once.\n",
    "While manual trial and error can be an effective technique to understand the\n",
    "impact of variable selection and hyperparameters, there are also automated\n",
    "techniques for model optimization, such as grid search. Grid search allows\n",
    "you to list a range of configurations you wish to test for each hyperparameter,\n",
    "and then methodically tests each of those possible hyperparameters. An\n",
    "automated voting process takes place to determine the optimal model. As the\n",
    "model must test each possible combination of hyperparameters, grid search\n",
    "does take a long time to run! Example code for grid search is shown at the\n",
    "end of this chapter.\n",
    "Finally, if you wish to use a different supervised machine learning algorithm\n",
    "and not gradient boosting, much of the code used in this exercise can be\n",
    "replicated. For instance, the same code can be used to import a new dataset,\n",
    "preview the dataframe, remove features (columns), remove rows, split and\n",
    "shuffle the dataset, and evaluate mean absolute error.\n",
    "http://scikit-learn.org is a great resource to learn more about other algorithms\n",
    "as well as the gradient boosting used in this exercise.\n",
    "For a copy of the code, please contact the author at\n",
    "oliver.theobald@scatterplotpress.com or see the code example below. In\n",
    "addition, if you have troubles implementing the model using the code found\n",
    "in this book, please feel free to contact the author by email for extra\n",
    "assistance at no cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for the Optimized Model\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_splitfrom sklearn import ensemble\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.externals import joblib\n",
    "# Read in data from CSV\n",
    "df = pd.read_csv('~/Downloads/Melbourne_housing_FULL-26-09-2017.csv')\n",
    "# Delete unneeded columns\n",
    "del df['Address']\n",
    "del df['Method']\n",
    "del df['SellerG']\n",
    "del df['Date']\n",
    "del df['Postcode']\n",
    "del df['Lattitude']\n",
    "del df['Longtitude']\n",
    "del df['Regionname']\n",
    "del df['Propertycount']\n",
    "# Remove rows with missing values\n",
    "df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "# Convert non-numerical data using one-hot encoding\n",
    "features_df = pd.get_dummies(df, columns=['Suburb', 'CouncilArea', 'Type'])\n",
    "# Remove price\n",
    "del features_df['Price']\n",
    "# Create X and y arrays from the dataset\n",
    "X = features_df.as_matrix()\n",
    "y = df['Price'].as_matrix()\n",
    "# Split data into test/train set (70/30 split) and shuffle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "# Set up algorithm\n",
    "model = ensemble.GradientBoostingRegressor(\n",
    "n_estimators=250,\n",
    "learning_rate=0.1,\n",
    "max_depth=5,\n",
    "min_samples_split=4,\n",
    "min_samples_leaf=6,\n",
    "max_features=0.6,\n",
    "loss='huber'\n",
    ")\n",
    "# Run model on training data\n",
    "model.fit(X_train, y_train)# Save model to file\n",
    "joblib.dump(model, 'trained_model.pkl')\n",
    "# Check model accuracy (up to two decimal places)\n",
    "mse = mean_absolute_error(y_train, model.predict(X_train))\n",
    "print (\"Training Set Mean Absolute Error: %.2f\" % mse)\n",
    "mse = mean_absolute_error(y_test, model.predict(X_test))\n",
    "print (\"Test Set Mean Absolute Error: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for Grid Search Model\n",
    "# Import libraries, including GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Read in data from CSV\n",
    "df = pd.read_csv('~/Downloads/Melbourne_housing_FULL-26-09-2017.csv')\n",
    "# Delete unneeded columns\n",
    "del df['Address']\n",
    "del df['Method']\n",
    "del df['SellerG']\n",
    "del df['Date']\n",
    "del df['Postcode']\n",
    "del df['Lattitude']\n",
    "del df['Longtitude']\n",
    "del df['Regionname']\n",
    "del df['Propertycount']\n",
    "# Remove rows with missing values\n",
    "df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)\n",
    "# Convert non-numerical data using one-hot encoding\n",
    "features_df = pd.get_dummies(df, columns=['Suburb', 'CouncilArea', 'Type'])\n",
    "# Remove price\n",
    "del features_df['Price']\n",
    "# Create X and y arrays from the dataset\n",
    "X = features_df.as_matrix()\n",
    "y = df['Price'].as_matrix()\n",
    "# Split data into test/train set (70/30 split) and shuffleX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "# Input algorithm\n",
    "model = ensemble.GradientBoostingRegressor()\n",
    "# Set the configurations that you wish to test\n",
    "param_grid = {\n",
    "'n_estimators': [300, 600, 1000],\n",
    "'max_depth': [7, 9, 11],\n",
    "'min_samples_split': [3, 4, 5],\n",
    "'min_samples_leaf': [5, 6, 7],\n",
    "'learning_rate': [0.01, 0.02, 0.6, 0.7],\n",
    "'max_features': [0.8, 0.9],\n",
    "'loss': ['ls', 'lad', 'huber']\n",
    "}\n",
    "# Define grid search. Run with four CPUs in parallel if applicable.\n",
    "gs_cv = GridSearchCV(model, param_grid, n_jobs=4)\n",
    "# Run grid search on training data\n",
    "gs_cv.fit(X_train, y_train)\n",
    "# Print optimal hyperparameters\n",
    "print(gs_cv.best_params_)\n",
    "# Check model accuracy (up to two decimal places)\n",
    "mse = mean_absolute_error(y_train, gs_cv.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.2f\" % mse)\n",
    "mse = mean_absolute_error(y_test, gs_cv.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.2f\" % mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FURTHER RESOURCES\n",
    "This section lists relevant learning materials for readers that wish to progress\n",
    "further in the field of machine learning. Please note that certain details listed\n",
    "in this section, including prices, may be subject to change in the future.\n",
    "| Machine Learning |\n",
    "Machine Learning\n",
    "Format: Coursera course\n",
    "Presenter: Andrew Ng\n",
    "Cost: Free\n",
    "Suggested Audience: Beginners (especially those with a preference for\n",
    "MATLAB)\n",
    "A free and well-taught introduction from Andrew Ng, one of the most\n",
    "influential figures in this field. This course has become a virtual rite of\n",
    "passage for anyone interested in machine learning.\n",
    "Project 3: Reinforcement Learning\n",
    "Format: Online blog tutorial\n",
    "Author: EECS Berkeley\n",
    "Suggested Audience: Upper intermediate to advanced\n",
    "A practical demonstration of reinforcement learning, and Q-learning\n",
    "specifically, explained through the game Pac-Man.\n",
    "| Basic Algorithms |\n",
    "Machine Learning With Random Forests And Decision Trees: A Visual\n",
    "Guide For Beginners\n",
    "Format: E-book\n",
    "Author: Scott Hartshorn\n",
    "Suggested Audience: Established beginnersA short, affordable (USD $3.20), and engaging read on decision trees and\n",
    "random forests with detailed visual examples, useful practical tips, and clear\n",
    "instructions.\n",
    "Linear Regression And Correlation: A Beginner's Guide\n",
    "Format: E-book\n",
    "Author: Scott Hartshorn\n",
    "Suggested Audience: All\n",
    "A well-explained and affordable (USD $3.20) introduction to linear\n",
    "regression, as well as correlation.\n",
    "| The Future of AI |\n",
    "The Inevitable: Understanding the 12 Technological Forces That Will\n",
    "Shape Our Future\n",
    "Format: E-Book, Book, Audiobook\n",
    "Author: Kevin Kelly\n",
    "Suggested Audience: All (with an interest in the future)\n",
    "A well-researched look into the future with a major focus on AI and machine\n",
    "learning by The New York Times Best Seller Kevin Kelly. Provides a guide\n",
    "to twelve technological imperatives that will shape the next thirty years.\n",
    "Homo Deus: A Brief History of Tomorrow\n",
    "Format: E-Book, Book, Audiobook\n",
    "Author: Yuval Noah Harari\n",
    "Suggested Audience: All (with an interest in the future)\n",
    "As a follow-up title to the success of Sapiens: A Brief History of Mankind,\n",
    "Yuval Noah Harari examines the possibilities of the future with notable\n",
    "sections of the book examining machine consciousness, applications in AI,\n",
    "and the immense power of data and algorithms.\n",
    "| Programming |\n",
    "Learning Python, 5th EditionFormat: E-Book, Book\n",
    "Author: Mark Lutz\n",
    "Suggested Audience: All (with an interest in learning Python)\n",
    "A comprehensive introduction to Python published by O’Reilly Media.\n",
    "Hands-On Machine Learning with Scikit-Learn and TensorFlow:\n",
    "Concepts, Tools, and Techniques to Build Intelligent Systems\n",
    "Format: E-Book, Book\n",
    "Author: Aurélien Géron\n",
    "Suggested Audience: All (with an interest in programming in Python, Scikit-\n",
    "Learn and TensorFlow)\n",
    "As a highly popular O’Reilly Media book written by machine learning\n",
    "consultant Aurélien Géron, this is an excellent advanced resource for anyone\n",
    "with a solid foundation of machine learning and computer programming.\n",
    "| Recommendation Systems |\n",
    "The Netflix Prize and Production Machine Learning Systems: An Insider\n",
    "Look\n",
    "Format: Blog\n",
    "Author: Mathworks\n",
    "Suggested Audience: All\n",
    "A very interesting blog article demonstrating how Netflix applies machine\n",
    "learning to form movie recommendations.\n",
    "Recommender Systems\n",
    "Format: Coursera course\n",
    "Presenter: The University of Minnesota\n",
    "Cost: Free 7-day trial or included with $49 USD Coursera subscription\n",
    "Suggested Audience: All\n",
    "Taught by the University of Minnesota, this Coursera specialization covers\n",
    "fundamental recommender system techniques including content-based and\n",
    "collaborative filtering as well as non-personalized and project-association\n",
    "recommender systems.\n",
    ".| Deep Learning |\n",
    "Deep Learning Simplified\n",
    "Format: Blog\n",
    "Channel: DeepLearning.TV\n",
    "Suggested Audience: All\n",
    "A short video series to get you up to speed with deep learning. Available for\n",
    "free on YouTube.\n",
    "Deep Learning Specialization: Master Deep Learning, and Break into AI\n",
    "Format: Coursera course\n",
    "Presenter: deeplearning.ai and NVIDIA\n",
    "Cost: Free 7-day trial or included with $49 USD Coursera subscription\n",
    "Suggested Audience: Intermediate to advanced (with experience in Python)\n",
    "A robust curriculum for those wishing to learn how to build neural networks\n",
    "in Python and TensorFlow, as well as career advice, and how deep learning\n",
    "theory applies to industry.\n",
    "Deep Learning Nanodegree\n",
    "Format: Udacity course\n",
    "Presenter: Udacity\n",
    "Cost: $599 USD\n",
    "Suggested Audience: Upper beginner to advanced, with basic experience in\n",
    "Python\n",
    "Comprehensive and practical introduction to convolutional neural networks,\n",
    "recurrent neural networks, and deep reinforcement learning taught online\n",
    "over a four-month period. Practical components include building a dog breed\n",
    "classifier, generating TV scripts, generating faces, and teaching a quadcopter\n",
    "how to fly.\n",
    "| Future Careers |\n",
    "Will a Robot Take My Job?\n",
    "Format: Online articleAuthor: The BBC\n",
    "Suggested Audience: All\n",
    "Check how safe your job is in the AI era leading up to the year 2035.\n",
    "So You Wanna Be a Data Scientist? A Guide to 2015's Hottest Profession\n",
    "Format: Blog\n",
    "Author: Todd Wasserman\n",
    "Suggested Audience: All\n",
    "Excellent insight into becoming a data scientist.\n",
    "The Data Science Venn Diagram\n",
    "Format: Blog\n",
    "Author: Drew Conway\n",
    "Suggested Audience: All\n",
    "The popular 2010 data science diagram designed by Drew Conway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOWNLOADING DATASETS\n",
    "Before you can start practicing algorithms and building machine learning\n",
    "models, you will first need data. For beginners starting out in machine\n",
    "learning, there are a number of options. One is to source your own dataset\n",
    "from writing a web crawler in Python or utilizing a click-and-drag tool such\n",
    "as Import.io to crawl the Internet. However, the easiest and best option to get\n",
    "started is by visiting kaggle.com.\n",
    "As mentioned throughout this book, Kaggle offers free datasets for\n",
    "download. This saves you the time and effort of sourcing and formatting your\n",
    "own dataset. Meanwhile, you also have the opportunity to discuss and\n",
    "problem-solve with other users on the forum, join competitions, and simply\n",
    "hang out and talk about data.\n",
    "Bear in mind, however, that datasets you download from Kaggle will\n",
    "inherently need some refining (through scrubbing) to tailor to the machine\n",
    "learning model that you decide to build. Below are four free sample datasets\n",
    "from Kaggle that may prove useful to your further learning in this field.\n",
    "World Happiness Report\n",
    "What countries rank the highest in overall happiness? Which factors\n",
    "contribute most to happiness? How did country rankings change between the\n",
    "2015 and 2016 reports? Did any country experience a significant increase or\n",
    "decrease in happiness? These are the questions you can ask of this dataset\n",
    "recording happiness scores and rankings using data from the Gallup World\n",
    "Poll. The scores are based on answers to the main life evaluation questions\n",
    "asked in the poll.\n",
    "Hotel Reviews\n",
    "Does having a five-star reputation lead to more disgruntled guests, and\n",
    "conversely, can two-star hotels rock the guest ratings by setting low\n",
    "expectations and over-delivering? Or are one and two-star rated hotels simply\n",
    "rated low for a reason? Find all this out from this sample dataset of hotel\n",
    "reviews. This particular dataset covers 1,000 hotels and includes hotel name,\n",
    "location, review date, text, title, username, and rating. The dataset is sourced\n",
    "from the Datafiniti’s Business Database, which includes almost every hotel inthe world.\n",
    "Craft Beers Dataset\n",
    "Do you like craft beer? This dataset contains a list of 2,410 American craft\n",
    "beers and 510 breweries collected in January 2017 from CraftCans.com.\n",
    "Drinking and data crunching is perfectly legal.\n",
    "Brazil's House of Deputies Reimbursements\n",
    "As politicians in Brazil are entitled to receive refunds from money spent on\n",
    "activities to “better serve the people,” there are interesting findings and\n",
    "suspicious outliers to be found in this dataset. Data on these expenses are\n",
    "publicly available, but there is very little monitoring of expenses in Brazil. So\n",
    "don’t be surprised to see one public servant racking up over 800 flights in\n",
    "twelve months, and another that recorded R 140,000 (USD $44,500) on post\n",
    "expenses—yes, snail mail!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
