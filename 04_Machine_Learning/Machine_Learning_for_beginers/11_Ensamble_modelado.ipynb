{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENSEMBLE MODELING\n",
    "One of the most effective machine learning methodologies is ensemble\n",
    "modeling, also known as ensembles. Ensemble modeling combines statistical\n",
    "techniques to create a model that produces a unified prediction. It is through\n",
    "combining estimates and following the wisdom of the crowd that ensemble\n",
    "modeling performs a final classification or outcome with better predictive\n",
    "performance. Naturally, ensemble models are a popular choice when it comes\n",
    "to machine learning competitions like the Netflix Competition and Kaggle\n",
    "competitions.\n",
    "Ensemble models can be classified into various categories including\n",
    "sequential, parallel, homogenous, and heterogeneous. Let’s start by first\n",
    "looking at sequential and parallel models. For sequential ensemble models,\n",
    "prediction error is reduced by adding weights to classifiers that previously\n",
    "misclassified data. Gradient boosting and AdaBoost are two examples of\n",
    "sequential models. Conversely, parallel ensemble models work concurrently\n",
    "and reduce error by averaging. Decision trees are an example of this\n",
    "technique.\n",
    "Ensemble models can also be generated using a single technique with\n",
    "numerous variations (known as a homogeneous ensemble) or through\n",
    "different techniques (known as a heterogeneous ensemble). An example of a\n",
    "homogeneous ensemble model would be numerous decision trees working\n",
    "together to form a single prediction (bagging). Meanwhile, an example of a\n",
    "heterogeneous ensemble would be the usage of k-means clustering or a neural\n",
    "network in collaboration with a decision tree model.\n",
    "Naturally, it is important to select techniques that complement each other.\n",
    "Neural networks, for instance, require complete data for analysis, whereas\n",
    "decision trees can effectively handle missing values. Together, these two\n",
    "techniques provide added value over a homogeneous model. The neural\n",
    "network accurately predicts the majority of instances that provide a value and\n",
    "the decision tree ensures that there are no “null” results that would otherwise\n",
    "be incurred from missing values in a neural network. The other advantage of\n",
    "ensemble modeling is that aggregated estimates are generally more accuratethan any single estimate.\n",
    "There are various subcategories of ensemble modeling; we have already\n",
    "touched on two of these in the previous chapter. Four popular subcategories\n",
    "of ensemble modeling are bagging, boosting, a bucket of models, and\n",
    "stacking.\n",
    "Bagging, as we know, is short for “boosted aggregating” and is an example\n",
    "of a homogenous ensemble. This method draws upon randomly drawn\n",
    "datasets and combines predictions to design a unified model based on a\n",
    "voting process among the training data. Expressed in another way, bagging is\n",
    "a special process of model averaging. Random forest, as we know, is a\n",
    "popular example of bagging.\n",
    "Boosting is a popular alternative technique that addresses error and data\n",
    "misclassified by the previous iteration to form a final model. Gradient\n",
    "boosting and AdaBoost are both popular examples of boosting.\n",
    "A bucket of models trains numerous different algorithmic models using the\n",
    "same training data and then picks the one that performed most accurately on\n",
    "the test data.\n",
    "Stacking runs multiple models simultaneously on the data and combines\n",
    "those results to produce a final model. This technique is currently very\n",
    "popular in machine learning competitions, including the Netflix Prize. (Held\n",
    "between 2006 and 2009, Netflix offered a prize for a machine learning model\n",
    "that could improve their recommender system in order to produce more\n",
    "effective movie recommendations. One of the winning techniques adopted a\n",
    "form of linear stacking that combined predictions from multiple predictive\n",
    "models.)\n",
    "Although ensemble models typically produce more accurate predictions, one\n",
    "drawback to this methodology is, in fact, the level of sophistication.\n",
    "Ensembles face the same trade-off between accuracy and simplicity as a\n",
    "single decision tree versus a random forest. The transparency and simplicity\n",
    "of a simple technique, such as a decision tree or k-nearest neighbors, is lost\n",
    "and instantly mutated into a statistical black-box. Performance of the model\n",
    "will win out in most cases, but the transparency of your model is another\n",
    "factor to consider when determining your preferred methodology."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
